<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>MMDetection &amp; object detection方面的调研</title>
    <link href="/2023/10/08/MMDetection/"/>
    <url>/2023/10/08/MMDetection/</url>
    
    <content type="html"><![CDATA[<h2 id="MMDetection-2D"><a href="#MMDetection-2D" class="headerlink" title="MMDetection(2D)"></a>MMDetection(2D)</h2><p>MMDetection is an open source object detection toolbox based on PyTorch. It consists of:</p><ul><li>Training recipes for object detection and instance segmentation.</li><li>360+ pre-trained models to use for fine-tuning 微调 (or training afresh).</li><li>Dataset support for popular vision datasets such as COCO, Cityscapes, LVIS and PASCAL VOC.</li></ul><p><a href="https://paperswithcode.com/lib/mmdetection">https://paperswithcode.com/lib/mmdetection</a></p><table><thead><tr><th></th><th align="center">MODEL</th><th align="center">TRANING ON</th><th align="center">BOX AP</th><th align="center">INFERENCETIME</th><th>PAPER</th><th>YEAR</th></tr></thead><tbody><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_1.00.08_PM_JkHBLyY.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/dynamic-r-cnn">Dynamic R-CNN</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">38.9</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_12.57.06_PM_O1JC9NU.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/detr">DETR</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">40.1</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_1.30.37_PM_Be7ErEb.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/groie">GRoIE</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">42.2</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/FASTER-RCNN_WhjSRSs.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/regnet">RegNet</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">43.1</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_2.25.41_PM_XDv3zNX.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/paa">PAA</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">45.1</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_2.51.06_PM_TdvT9xn.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/sparse-r-cnn">Sparse R-CNN</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">46.2</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-06_at_12.30.12_PM.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/resnest">ResNeSt</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">47.7</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_12.53.32_PM_GHe6eIp.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/detectors">DetectoRS</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">49.1</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_3.03.28_PM_R0DaAzb.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/vfnet">VFNet</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">50.4</td><td align="center"></td><td></td><td>2020</td></tr><tr><td><img src="https://production-media.paperswithcode.com/models/Screen_Shot_2021-02-24_at_1.23.54_PM_XwE0mAh.png" alt="img"></td><td align="center"><a href="https://paperswithcode.com/lib/mmdetection/generalized-focal-loss">Generalized Focal Loss</a></td><td align="center"><a href="https://paperswithcode.com/dataset/coco">COCO</a></td><td align="center">48.1</td><td align="center">0.0935</td><td></td><td>2020</td></tr></tbody></table><p><a href="https://github.com/open-mmlab/mmdetection3d">https://github.com/open-mmlab/mmdetection3d</a></p><p>[<img src="/img/figure/mmdet3d-logo.png" alt="mmdet3d-logo"></p><p><strong>OpenMMLab website</strong> <a href="https://openmmlab.com/"><em>HOT</em> </a>   <strong>OpenMMLab platform</strong> <a href="https://platform.openmmlab.com/"><em>TRY IT OUT</em></a></p><p><a href="https://mmdetection3d.readthedocs.io/en/latest/"><img src="https://camo.githubusercontent.com/d5d535f53f2cb047c2b4382b8fd3c2913519abad35badcd4f22bd45d174f450a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d6c61746573742d626c7565" alt="docs"></a> <a href="https://github.com/open-mmlab/mmdetection3d/actions"><img src="https://github.com/open-mmlab/mmdetection3d/workflows/build/badge.svg" alt="badge"></a> <a href="https://codecov.io/gh/open-mmlab/mmdetection3d"><img src="https://camo.githubusercontent.com/9d9dd5bbee06fe143d72c2a2f1907aaf74c2a68c716fcaa52e3cbcb9f8677033/68747470733a2f2f636f6465636f762e696f2f67682f6f70656e2d6d6d6c61622f6d6d646574656374696f6e33642f6272616e63682f6d61737465722f67726170682f62616467652e737667" alt="codecov"></a> <a href="https://github.com/open-mmlab/mmdetection3d/blob/master/LICENSE"><img src="https://camo.githubusercontent.com/aa0f4ed735ed4dbdbbc25ab552ee5d61089224d21ba1c545d7c16e0fa94da8d8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6f70656e2d6d6d6c61622f6d6d646574656374696f6e33642e737667" alt="license"></a></p><h2 id="MMDetection3D"><a href="#MMDetection3D" class="headerlink" title="MMDetection3D"></a>MMDetection3D</h2><p>MMDetection3D is an open source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D detection. It is a part of the OpenMMLab project developed by <a href="http://mmlab.ie.cuhk.edu.hk/">MMLab</a>.（支持分布式训练和推理；it supports distributed training and inference.）</p><p><img src="/img/figure/mmdet3d_outdoor_demo.gif" alt="demo image"></p><h3 id="Major-features"><a href="#Major-features" class="headerlink" title="Major features"></a>Major features</h3><ul><li><p><strong>Support multi-modality&#x2F;single-modality detectors out of box</strong></p><p>It directly supports multi-modality(多模态)&#x2F;single-modality(单模态) detectors including MVXNet, VoteNet, PointPillars, etc.</p></li><li><p><strong>Support indoor&#x2F;outdoor 3D detection out of box</strong></p><p>It directly supports popular indoor and outdoor 3D detection datasets, including ScanNet, SUNRGB-D, Waymo, nuScenes, Lyft, and KITTI. For nuScenes dataset, we also support <a href="https://github.com/open-mmlab/mmdetection3d/tree/latest/configs/nuimages">nuImages dataset</a>.</p></li><li><p><strong>Natural integration with 2D detection</strong></p><p>All the about <strong>300+ models, methods of 40+ papers</strong>, and modules supported in <a href="https://github.com/open-mmlab/mmdetection/blob/3.x/docs/en/model_zoo.md">MMDetection</a> can be trained or used in this codebase.</p></li><li><p><strong>High efficiency</strong></p><p>It trains faster than other codebases. The main results are as below. Details can be found in <a href="https://github.com/open-mmlab/mmdetection3d/blob/main/docs/en/notes/benchmarks.md">benchmark.md</a>. We compare the number of samples trained per second (the higher, the better). The models that are not supported by other codebases are marked by <code>✗</code>.</p><table><thead><tr><th>Methods</th><th>MMDetection3D</th><th><a href="https://github.com/open-mmlab/OpenPCDet">OpenPCDet</a></th><th><a href="https://github.com/facebookresearch/votenet">votenet</a></th><th><a href="https://github.com/poodarchu/Det3D">Det3D</a></th></tr></thead><tbody><tr><td>VoteNet</td><td>358</td><td>✗</td><td>77</td><td>✗</td></tr><tr><td>PointPillars-car</td><td>141</td><td>✗</td><td>✗</td><td>140</td></tr><tr><td>PointPillars-3class</td><td>107</td><td>44</td><td>✗</td><td>✗</td></tr><tr><td>SECOND</td><td>40</td><td>30</td><td>✗</td><td>✗</td></tr><tr><td>Part-A2</td><td>17</td><td>14</td><td>✗</td><td>✗</td></tr></tbody></table></li></ul><p>Like <a href="https://github.com/open-mmlab/mmdetection">MMDetection</a> and <a href="https://github.com/open-mmlab/mmcv">MMCV</a>, MMDetection3D can also be used as a library to support different projects on top of it.</p><h3 id="Benchmarks"><a href="#Benchmarks" class="headerlink" title="Benchmarks"></a>Benchmarks</h3><p>Here we benchmark the training and testing speed of models in MMDetection3D, with some other open source 3D detection codebases.</p><h4 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h4><ul><li>Hardwares: 8 NVIDIA Tesla V100 (32G) GPUs, Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz<br>Software: Python 3.7, CUDA 10.1, cuDNN 7.6.5, PyTorch 1.3, numba 0.48.0.</li><li>Model: Since all the other codebases implements different models, we compare the corresponding models including SECOND, PointPillars, Part-A2, and VoteNet with them separately.</li><li>Metrics: We use the average throughput in iterations of the entire training run and skip the first 50 iterations of each epoch to skip GPU warmup time.</li></ul><h4 id="Main-Results"><a href="#Main-Results" class="headerlink" title="Main Results"></a>Main Results</h4><p>We compare the training speed (samples&#x2F;s) with other codebases if they implement the similar models. The results are as below, <strong>the greater the numbers in the table, the faster of the training process</strong>. The models that are not supported by other codebases are marked by <code>×</code>.</p><table><thead><tr><th>Methods</th><th>MMDetection3D</th><th>OpenPCDet</th><th>votenet</th><th>Det3D</th></tr></thead><tbody><tr><td>VoteNet</td><td>358</td><td>×</td><td>77</td><td>×</td></tr><tr><td>PointPillars-car</td><td>141</td><td>×</td><td>×</td><td>140</td></tr><tr><td>PointPillars-3class</td><td>107</td><td>44</td><td>×</td><td>×</td></tr><tr><td>SECOND</td><td>40</td><td>30</td><td>×</td><td>×</td></tr><tr><td>Part-A2</td><td>17</td><td>14</td><td>×</td><td>×</td></tr></tbody></table><p>In summary, MMdetection is a toolbox for object detection, MMCV is a foundation library(基础库) for computer vision research that supports MMDetection, and MMDetection3D is an extension of MMDetection for 3D object detection and tracking.</p><p>总之，MMDetection是一个物体检测的工具箱，MMCV是一个支持MMDetection的计算机视觉研究的基础库，而MMDetection3D是MMDetection的扩展，用于3D物体检测和跟踪。</p><h3 id="Verify-the-installation"><a href="#Verify-the-installation" class="headerlink" title="Verify the installation"></a>Verify the installation</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># single-gpu testing</span><br>python tools/test.py <span class="hljs-variable">$&#123;CONFIG_FILE&#125;</span> <span class="hljs-variable">$&#123;CHECKPOINT_FILE&#125;</span> [--cfg-options test_evaluator.pklfile_prefix=<span class="hljs-variable">$&#123;RESULT_FILE&#125;</span>]  [--show] [--show-dir <span class="hljs-variable">$&#123;SHOW_DIR&#125;</span>]<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># e.g.</span><br>python demo/pcd_demo.py demo/data/kitti/000008.bin pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py hv_pointpillars_secfpn_6x8_160e_kitti-3d-car_20220331_134606-d42d15ed.pth --show<br></code></pre></td></tr></table></figure><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">python tools<span class="hljs-regexp">/test.py configs/</span>votenet<span class="hljs-regexp">/votenet_8xb8_scannet-3d.py checkpoints/</span>votenet_8x8_scannet-<span class="hljs-number">3</span>d-<span class="hljs-number">18</span>class_20200620_230238-<span class="hljs-number">2</span>cea9c3a.pth --show --show-dir .<span class="hljs-regexp">/data/</span>scannet/show_results<br></code></pre></td></tr></table></figure><ul><li><p><input disabled="" type="checkbox"> VoteNet </p></li><li><p><input checked="" disabled="" type="checkbox"> PointPillars</p></li><li><p><input disabled="" type="checkbox"> SECOND</p></li><li><p><input disabled="" type="checkbox"> Part-A2</p></li></ul><h3 id="雷达参数"><a href="#雷达参数" class="headerlink" title="雷达参数"></a>雷达参数</h3><blockquote><p>目前大车使用的镭神智能的雷达参数</p></blockquote><p><img src="https://pic1.zhimg.com/v2-c37643d43854f555e89cc3f3e5aae138_r.jpg" alt="img"></p><blockquote><p>测绘激光雷达三维测绘激光雷达 MS-C16</p></blockquote><table><thead><tr><th align="center">大场景快速建模</th><th align="center">近程高精建图</th></tr></thead><tbody><tr><td align="center"><img src="https://www.leishen-lidar.com/public/attachment/images/20210525/833cf8d7b65d2925469d07b584a1eee2.jpg" alt="img"></td><td align="center"><img src="https://www.leishen-lidar.com/public/attachment/images/20210525/ec827ed4b51ce3d2c38122cf071dcde5.jpg" alt="img"></td></tr></tbody></table><blockquote><p>镭神智能128 线混合固态激光雷达 CH128 效果</p></blockquote><table><thead><tr><th align="center">非机动车</th><th align="center">轿车</th></tr></thead><tbody><tr><td align="center"><img src="https://www.leishen-lidar.com/public/attachment/images/20210430/e8cddbf06dc62fd428b3c514568733c0.jpg" alt="img" style="zoom:25%;" /></td><td align="center"><img src="https://www.leishen-lidar.com/public/attachment/images/20210430/d375483090b6c825e5db230913d94b5c.jpg" alt="img" style="zoom:25%;" /></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>notebook</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Autopilot</tag>
      
      <tag>object detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PointPillars 论文笔记</title>
    <link href="/2023/10/08/PointPillars/"/>
    <url>/2023/10/08/PointPillars/</url>
    
    <content type="html"><![CDATA[<h2 id="PointPillars"><a href="#PointPillars" class="headerlink" title="PointPillars"></a>PointPillars</h2><blockquote><p><a href="D:\中国科学院软件研究所\自动驾驶\相关文献\PointPillars.pdf">PointPillars.pdf</a> </p><p>PointPillar的网络结构</p><p>(D, P, N)–&gt; (C, P, N) –&gt; (C, P) –&gt; (C, H, W) –&gt; (6C, H&#x2F;2, W&#x2F;2) –&gt; bbox</p></blockquote><p><img src="/img/figure/pointpillars_1.png" alt="pointpillars_1"></p><ol><li>提出一种新的encoding points的方式: Pillar</li><li>fast version–  平均62Hz , faster version– 105 Hz, 超过SECOND三倍。(激光雷达工作频率通常是5HZ&#x2F;10HZ&#x2F;20Hz，105HZ的LIDAR现实中比较少)</li><li>对于模型能达到的速度和真实自动驾驶场景能达到的速度做了一些讨论</li></ol><ul><li><p>PointPillars-car:</p><p>PointPillars-car是专门用于检测汽车的模型，它只需要检测汽车这一种目标类别，因此输出结果只包含汽车的检测框和相关的属性信息</p></li><li><p>PointPillars-3class:</p><p>PointPillars-3class则可以检测三种目标类别，包括汽车、行人和自行车，因此输出结果会同时包含这三种目标的检测框和相关属性信息。</p></li></ul><h3 id="Pillar-方式编码"><a href="#Pillar-方式编码" class="headerlink" title="Pillar 方式编码"></a>Pillar 方式编码</h3><p><strong>(D, P, N)–&gt; (C, P, N) –&gt; (C, P) –&gt; (C, H, W) –&gt; (6C, H&#x2F;2, W&#x2F;2) –&gt; bbox</strong></p><p><strong>– 张量化</strong></p><p>point clouds –&gt; (D, P, N)</p><blockquote><p>p : max number of pillars (P)，最大pillars (P) 具体使用 12000<br>N : max number of points per pillar (N)，每个pillars的最大点数 (N), 具体使用100<br>D : 是 dimension</p><p>点云的表示(D&#x3D;9 dimension)：</p></blockquote><p>$$<br>(x,y,z,r,x_c,y_c,z_c,x_p,y_p)<br>$$</p><p>$$ x,y,z$$为点云的真实坐标信息和反射强度,$$x_c,y_c,z_c$$为该点云所处Pillar中的所有点的几何中心；$$x_p,y_p$$为$$x-x_c,y-y_c$$,反应了点与几何中心的相对位置。</p><p><img src="/img/figure/pointpillars_2.png" alt="pointpillars_2"></p><ul><li>特征提取网络：应用线性层(linear layer)，然后应用 BatchNorm 和 ReLU 以生成 (C, P, N) 大小的<br>张量。</li></ul><blockquote><p>原来的维度是D&#x3D;9，经过Pillar Feature Net（<strong>特征提取网络</strong>）后，得到新的维度C，(D, P, N) –&gt; (C, P, N）</p><p>按照Pillar所在维度进行Max Pooling操作，即获得了(C, P)维度的特征图。</p><p>特征被编码后，特征会被分散到原来的pillar的位置，创建一个尺寸为(C, H, W)，其中H和W表示画布的长度和高度。</p></blockquote><p><strong>– Backbone (2D CNN)</strong></p><ul><li>指在输入图像上进行特征提取的网络核心结构。</li><li>Backbone 通常由一系列卷积层组成，通常以分层结构组织，学习检测输入图像中越来越复杂和抽象的特征。然后，这些特征被传递给一个或多个执行分类任务的全连接层。</li><li>Backbone 结构的选择对网络的性能有很大的影响，包括准确性和计算效率方面。流行的二维CNN的骨干架构包括VGG、ResNet、Inception和MobileNet等。   </li><li>在许多情况下，骨干结构可以在一个大的数据集上进行预训练，如ImageNet，以学习一套通用的特征，这些特征可以为特定的任务进行微调。这种方法被称为迁移学习，在为特定的应用开发和训练一个新的CNN时可以节省大量的时间和计算资源。</li><li><img src="/img/figure/pointpillars_3.png" alt="pointpillars_3"></li><li>mmdetection3D中针对KITTi数据集使用的Backbone是SECFN，下面只放KITTI的结果，其他结果(nuScenes&#x2F;Lyft&#x2F;Waymo)详见<a href="https://github.com/open-mmlab/mmdetection3d/blob/main/configs/pointpillars/README.md">https://github.com/open-mmlab/mmdetection3d/blob/main/configs/pointpillars/README.md</a></li><li><h4 id="Results-and-models"><a href="#Results-and-models" class="headerlink" title="Results and models"></a>Results and models</h4><h5 id="KITTI"><a href="#KITTI" class="headerlink" title="KITTI"></a>KITTI</h5><table><thead><tr><th>Backbone</th><th>Class</th><th>Lr schd</th><th>Mem (GB)</th><th>Inf time (fps)</th><th>AP</th><th>Download</th></tr></thead><tbody><tr><td><a href="https://github.com/open-mmlab/mmdetection3d/blob/main/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car.py">SECFPN</a></td><td>Car</td><td>cyclic 160e</td><td>5.4</td><td></td><td>77.6</td><td><a href="https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_secfpn_6x8_160e_kitti-3d-car/hv_pointpillars_secfpn_6x8_160e_kitti-3d-car_20220331_134606-d42d15ed.pth">model</a> | <a href="https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_secfpn_6x8_160e_kitti-3d-car/hv_pointpillars_secfpn_6x8_160e_kitti-3d-car_20220331_134606.log.json">log</a></td></tr><tr><td><a href="https://github.com/open-mmlab/mmdetection3d/blob/main/configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class.py">SECFPN</a></td><td>3 Class</td><td>cyclic 160e</td><td>5.5</td><td></td><td>64.07</td><td><a href="https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth">model</a> | <a href="https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306.log.json">log</a></td></tr></tbody></table></li></ul><p> <strong>(C, H, W) –&gt; (6C, H&#x2F;2, W&#x2F;2)</strong></p><p><img src="/img/figure/pointpillars_4.png" alt="pointpillars_4"></p><ul><li><p>第一个网络：自上而下的网络以越来越小的空间分辨率产生特征，同时提升特征图的维度</p></li><li><p>第二个网络：对自上而下的特征进行上采样和串联。</p><p>之所以选择这样架构，是因为<strong>不同分辨率的特征图负责不同大小物体的检测</strong>。比如分辨率大的特征图往往感受野较小，适合捕捉小物体（在KITTI中就是行人）。</p></li></ul><p><strong>– Detection Head</strong></p><blockquote><p>文章中使用后Single Shot Detector（SSD）进行三维物体的检测设置，类似于SSD</p><ul><li>使用二维联合交集（IoU）将先验框和ground truth相匹配</li><li>boundingbox的高度H不用于匹配，相反给定一个二维的匹配，使高度和仰角成为额外的回归目标</li></ul></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 二维联合交集（IoU）的解释   </span><br>    在计算机视觉中，二维交叉联合可以用来进行目标检测、图像匹配、特征提取等任务。该方法将一个小的二维核（通常为正方形或矩形）在图像上滑动，计算核与图像重叠部分的像素值乘积的总和，得到一个输出矩阵。这个输出矩阵可以被解释为原始图像中与核最相似的区域。<br>    二维交叉联合可以用来检测图像中的边缘、纹理、角点等特征。在深度学习中，卷积神经网络（Convolutional Neural Network，CNN）中的卷积层实际上就是应用了二维交叉联合的操作。<br><span class="hljs-comment"># Conv是卷积convolutional的缩写，Deconv是去卷积操作</span><br>Conv layer常用于图像分类，物体检测和分割，这些层对输入数据进行卷积运算。<br>Deconv layer 被用于图像的生成和修复任务，进行反向的卷积操作，以便生成一个大于输入的输出，去卷积层通常也被称为转置的卷积层或者上采样层(upsampling)<br></code></pre></td></tr></table></figure><p><strong>Network</strong></p><p>$S$:相对于原始输入伪图像的测量值</p><ul><li><p>The encoder network jas C &#x3D; 64 output features, </p></li><li><p>First block (S &#x3D; 2 for car, S &#x3D; 1 for pedestrian&#x2F;cyclist).</p></li><li><p>Both network consists of three blocks, Block1(S, 4, C), Block2(2S, 6, 2C), and Block3(4S, 6, 4C).</p></li><li><p>three blocks,每个区块采用上采样步骤进行升采样，Up1(S, S, 2C), Up2(2S, S, 2C),  and Up3(4S, S, 2C),然后，三个区块被串联起来，形成Detection Head的6C特征</p></li></ul><p><strong>Loss funtion</strong></p><p>PointPillar use the same loss function  introduced in SECOND.</p><p>PointPillar的loss function和SECOND相似，每个3D的Boundingbox用一个7维的向量表示，分别为($x,y,z,w,h,l,\theta $),其中($x,y,z$)为中心，($w,h,l$)为尺寸数据，$\theta$为方向角。</p><p>检测框回归任务中要学习的参数维这7个变量的偏移量：<br>$\Delta x&#x3D;\frac{x^{gt}-x^{a}}{d^{a}}$；$\Delta y&#x3D;\frac{y^{gt}-y^{a}}{d^{a}}$；$\Delta z&#x3D;\frac{z^{gt}-z^{a}}{h^{a}}$；<br>$\Delta w&#x3D;log\frac{w^{gt}}{w^{a}}$$；\Delta l&#x3D;log\frac{l^{gt}}{l^{a}}$；$\Delta \theta&#x3D;sin(\theta^{gt}-\theta^{a})$</p><ul><li><p>$x^{gt}$:ground truth</p></li><li><p>$x^{a}$:anchor boxes(锚定框)</p></li><li><p>$d^{a}&#x3D;\sqrt{(w^{a})^{2}+(l^{a})^{2}} $</p></li></ul><p>$$<br>  L_loc&#x3D;\sum_{b\in (x,y,z,w,l,h,\theta)}SmoothL1(\Delta b)<br>$$</p><p>由于角度定位损失不能区分翻转的boxes，所以文章使用了一个关于离散化方向的softmax分类损失，$L_dir$(离散方向)，使得网络能够学习到heading的方向。</p><ul><li><p>对于object classification loss,使用focal loss:<br>$$<br>L_cls&#x3D;-\alpha_a(1-\beta^{a})^{\gamma}logp^{a}<br>$$</p></li><li><p>$p^{\alpha}$为锚点为某个类的可能性（置信度），其中$\alpha&#x3D;0.25$$\gamma&#x3D;2$，最终损失函数为：<br>$$<br>L&#x3D;\frac{1}{N_pos}(\beta_locL_loc+\beta_clsL_cls+\beta_dirL_dir)<br>$$</p></li><li><p>其中$N_pos$是positive anchors的数量，$\beta_los&#x3D;2$,$\beta_cls&#x3D;1$,$、beta_dir&#x3D;0.2$</p></li></ul><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><ul><li>PointPillar only train on lidar point clouds in KITTI dataset, but compare with fusion methods that use both lidar</li></ul>]]></content>
    
    
    <categories>
      
      <category>notebook</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Autopilot</tag>
      
      <tag>Adversarial Attack</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo部署到github.io中遇到的问题及解决方法</title>
    <link href="/2023/10/08/Hexo%E9%83%A8%E7%BD%B2%E5%88%B0github.io%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"/>
    <url>/2023/10/08/Hexo%E9%83%A8%E7%BD%B2%E5%88%B0github.io%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>解决<code>hexo -d </code>部署后与<code>github.io</code>不同步问题</p><blockquote><p>github.io中应避免出现两个及以上的branch，建议保留master</p></blockquote><p><code>hexo - d</code> 部署到<code>github.io</code> 时，hexo会根据<code>_config.yml</code>下的<code>Deployment</code>寻找github中的<code>branch</code>，多数教程中涉及在<code>Deployment</code>末尾加入<code>_config.yml</code>比如：</p><figure class="highlight plaintext"><figcaption><span>Deployment</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs #"># Deployment<br>## Docs: https://hexo.io/docs/one-command-deployment<br>deploy:<br>  type: git<br>  repo: https://github.com/memory009/memory009.github.io.git<br>  branch: master<br></code></pre></td></tr></table></figure><p>注意：</p><ul><li>此处添加的branch为<code>master</code>分支，最新的github将默认分支改为了<code>main</code>，所以在Git bash中进入blog目录之后使用<code>hexo deploy </code>部署是无效的</li></ul><p>解决办法：</p><ul><li>在github.io中<code>view all branches</code>, 将<code>main</code>分支删掉，将<code>master</code>分支设置为default(因为hexo出现的时间比较早，早期的github默认的主分支是<code>master</code>)</li><li>进入<code>settings</code> – &gt; <code>pages</code> 重新将根域名配置到 <code>Custom domain</code>, 等待几分钟后，根域名会通过<code>CNAME</code>指向github.io，实现在根域名直接看到github.io的效果</li></ul><p>测试方法：<br>在管理员权限下<code>Git bash</code>进入blog目录下</p><ul><li><code>$ hexo -clean </code>  : 清理缓存(慎用，会将blog&#x2F;public文件夹内全部东西恢复默认)</li><li><code>$ hexo generate </code>   : 生成静态网站</li><li><code>$ hexo -deploy</code> : 部署静态网站</li><li><code>$ hexo server  </code> : 在本地预览静态网站</li></ul>]]></content>
    
    
    <categories>
      
      <category>Environment configuration</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hexo</tag>
      
      <tag>github</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/10/07/hello-world/"/>
    <url>/2023/10/07/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
